# 1701QCA Final project journal: *Tess King*

<!--- As for other assessments, fill out the following journal sections with information relevant to your project. --->

<!--- Markdown reference: https://guides.github.com/features/mastering-markdown/ --->

### Aeolus ###


## Related projects ##


### Related project 1 ###

#### Jamboxx ####

*https://www.jamboxx.com/*

![jamboxx](https://user-images.githubusercontent.com/62095800/79901409-1fddc100-8453-11ea-8c0a-a6ad9c8214b0.png)

This project is related to mine because of the method of sound generation and creation. 


### Related project 2 ###

#### TEC USB MIDI Breath Controller ####

*https://www.tecontrol.se/products/usb-midi-breath-controller*

![tec_MIDI](https://user-images.githubusercontent.com/62095800/79901401-1d7b6700-8453-11ea-85d0-894bc6f7893a.jpg)

The TEC MIDI breath controller was the industry pioneer of digital breath controllers in the professional world. It's related due to the overall design concept and the conceptual model. 


### Related project 3 ###

#### Mi.Mu Gloves by Imogen Heap ####

*https://mimugloves.com/*

![mi mu](https://user-images.githubusercontent.com/62095800/79901412-20765780-8453-11ea-8bb9-9e42fff4bebe.jpg)

The Mi.Mu gloves are based on expression through intricate gesture of the hands. I found it relevant to look at how others approached the task of complex expression through one specific type of gesture. 


### Related project 4 ###

#### Talk Box ####

*https://en.wikipedia.org/wiki/Talk_box*

![talkbox](https://user-images.githubusercontent.com/62095800/79901394-1c4a3a00-8453-11ea-8847-7ca6076e8a7b.jpg)

This "vintage" product is very similar to my final conception in terms of materials and fabrication. 


### Related project 5 ###

#### Ohmerometer by Helen Collard and Alistair MacDonald ####

*https://catchyourbreath.org/kids-in-museums-takeover-day/ 1:19 to 2:38*

![ohmerometerll](https://user-images.githubusercontent.com/62095800/79901413-210eee00-8453-11ea-8028-a42f99809800.jpg)

The relevance here relates more to the concept of my design. Through an interactive art piece Ohmerometer was able to reach a wider and more receptive audience and still showcases the basic model. 


### Related project 6 ###

#### Roland - Aerophone AE-10 ####

*https://www.roland.com/au/products/aerophone_ae-10/*

![roland_aerophone](https://user-images.githubusercontent.com/62095800/79901416-22d8b180-8453-11ea-9623-8c34202076e0.jpg)

Here, Roland's product is a highly sophisticated fabrication but very similar in conceptual model, design and basic technical flow. 


## Other research ##


### ASPECTS OF GESTURE IN DIGITAL MUSICAL INSTRUMENT DESIGN - William Brent ###

*https://pdfs.semanticscholar.org/7a6f/95d18561c4d190e10b93f6a9a34cbb39a99d.pdf?_ga=2.30290047.1790675502.1587476728-755521967.1587022436*

This resource goes into detail of the psychology of gesture as part of the relationship between performer and instrument. Brent especially delves into design practices in the creation of digital instruments. His suggestions also compare to the response that acoustic instruments garner from users and audiences alike. 


### DIY MIDI Breath Controller ###

*http://synthhacker.blogspot.com/2019/09/diy-midi-breath-controller.html*

This DIY journal and guide is the main inspiration and technical research I've found. In it, the author describes their process for creating this MIDI controller for their synthesizers. He delves into the technical aspect of realising his ideation. In particular, providing schematics of his electronic setup and coding. He is also honest about the difficulties he had along the way, as well as the outcome. 



## Conceptual development ##

### Design intent ###
As a general and condensed statement: 

*To explore breath as expression*

### Design ideation ###

### Design concept 1 ###

#### A Creative Tool or Installation for Abstract Art ####

This would be an interactive art piece in which there is a large canvas and one or more tubes (with mouth-pieces) which connected to the canvas frame. Viewers would be encouraged to blow into the mouthpieces. This action would result in changes in what would appear on this canvas, depending on the length, interval and intensity of breath. Which would result in the formation of blobs varying in size, shape and colour corresponding to those variables. This would be similar to how Suminagashi (see here https://www.youtube.com/watch?v=qwaLFLbqJQg for an example) looks. An idea for technical execution was to either utilise a pre-existing screen or create once out of a matrix of strip LEDs or similar. 

There was a variation of this concept that I had. Instead of mouthpieces and tubes, viewers would blow directly onto the "canvas". Technically, this could be achieved with sensors hidden in areas of the screen (far more suitable to the second fabrication concept) which measured the breath.


### Design concept 2 ###

#### The Calm-Down Friend ####

This idea realises the intent by giving users a means to consider their breath more intently. It would be a compact box or similar object which the user breathes into. From there, the box will tell the user if they should slow their breathing down, deepen their breath or hold it for longer. This is intended to assist people in meditative practices and anxiety control methods. The interaction with the user would be some light and sound feedback. It could be sampled speech or ambient music which changes as your breath slows and portrays a calmer nature.

### Final design concept ###

#### Aeolus ####

A self contained musical instrument which is blown into in order to initiate musical feedback.  

This is a product which is designed for users to better understand the expressivity of breath through musical feedback. It is designed around affordances and mappings of acoustic wind instruments and geometrical shapes. 

The sound generated is less of my interest than the interactions which produce them and the feedback resulted. These interactions are focused around the breath and the gestures it contributes to in other areas of the body. The interactions capable with Aeolus are: breathing into the mouthpiece, holding it (preferably at the hand holds) and moving it (preferably from a central pivoting point at the mouth). This simplistic interaction model is designed with the intent that as users experiment more with the instrument, they naturally grow more aware of the movement of the entire body due to the breath and the subtleties which drastically change the sound. Or more simply: to draw attention to the expressivity of breath through sound expression. While the sound generation is a vast world of expression and feedback for the user, I felt that adding visual responses added personability to the instrument itself. Being an instrument which is an extension of the human body, this personability will aid viewers experience a deeper expression from the performer. 

### Interaction flowchart ###

![InteractiveDiagram](https://user-images.githubusercontent.com/62095800/82579964-e0c99980-9bd1-11ea-9e95-4992122b5f31.jpg)

## Process documentation ##

# STAGES FROM EARLIER POINTS #

## Physical experimentation documentation ##

### First Draft of Design ###

![first_draft](https://user-images.githubusercontent.com/62095800/79901528-4ac81500-8453-11ea-9f24-162215a79294.jpg)

Just a simple hand drawing of a rough idea at the very beginning of the whole process.


### Paper Net 1 ###

![papernet](https://user-images.githubusercontent.com/62095800/79902583-1190a480-8455-11ea-9781-97682f883420.jpg)

This was the first physical creation of my 3D model. I found free software that created a paper net of the model so that you could cut, fold and glue where indicated to achieve the 3D model in real life. This is especially useful for someone like myself, who is quite bad at paper crafts in general and terrible at measuring, drawing in a straight line and folding specifically.


### Paper Construction ###

![paper](https://user-images.githubusercontent.com/62095800/79902557-06d60f80-8455-11ea-94c0-2e7a5a26b9e9.jpg)

![paper (2)](https://user-images.githubusercontent.com/62095800/79902551-0473b580-8455-11ea-8e6b-d01e753e7637.jpg)

![paper_held](https://user-images.githubusercontent.com/62095800/79902562-09d10000-8455-11ea-9fb5-4034f5d2972a.jpg)

These are a few images of the paper net folded and taped together as a full 3D shape. I couldn't get the curving pieces in the hand holds to stick to the full model. The third image shows someone holding it as intended for context. At this point, I found that the scale I chose was far too small and was causing discomfort due to the angle of the wrists and elbows.


### Paper to Cardboard ###

![paper_to_cardboard](https://user-images.githubusercontent.com/62095800/79902571-0dfd1d80-8455-11ea-8319-a2ff168d907e.jpg)

Here is an expanded version of the paper model overlayed on B flute cardboard, ready to be cut. At this point, I really wanted to move onto a material that was far more durable and sturdy to work with. The scale was upped from X2u to X3.5u.


### Cardboard Iteration I ###

![cardboard_l](https://user-images.githubusercontent.com/62095800/79901473-3a179f00-8453-11ea-89f6-4666d49fa48c.jpg)

![cardboard_l (2)](https://user-images.githubusercontent.com/62095800/79901461-371cae80-8453-11ea-8eb9-f4146700263b.jpg)

This shows my very first attempt at cardboard construction of the form. I immediately found an issue. As seen in the second image, the apex points of the front and back were misaligned due to a measurement error. I changed my methodology after this.


### Cardboard Iteration II ###

![cardboard_ll](https://user-images.githubusercontent.com/62095800/79901484-3e43bc80-8453-11ea-99b8-c3fec187cf44.jpg)

![cardboard_ll_held](https://user-images.githubusercontent.com/62095800/79901499-43087080-8453-11ea-8e81-50b9d75e008f.jpg)

This is the second iteration of cardboard construction. This version is far more successful than the previous. The feedback I got was still that the handholds were a little uncomfortable.



### Cardboard Iteration III ###

![cardboard_lll](https://user-images.githubusercontent.com/62095800/79901511-47348e00-8453-11ea-80f6-268a94cae078.jpg)

This is the final iteration that I constructed for this journal. My changes to the hand holds were well received but felt that there was still room for improvement there. I intend to up the scale further and alter the width of the construction. 


### Technical Testing - Proof of Concept - Light ###

![makecode_proofofconcept](https://user-images.githubusercontent.com/62095800/79901531-4b60ab80-8453-11ea-9f2b-bb6db09ea7c3.png)

This is basic code using the Mircobit that's designed to test the concept of breath controlling sound and light based feedback.

![technical_concept_proof](https://user-images.githubusercontent.com/62095800/79901445-308e3700-8453-11ea-83fa-2e2e1cd7d9f7.jpg)

Here is the electronic circuit that goes along with this basic code. The video testing is available here: https://youtu.be/sq_ULI16xQg


### Technical Testing - Proof of Concept - Light & Sound ###

This test was quite useful in seeing the very basic principles in action, no matter how "low quality" the result was. The video which shows this cohesive proof of concept and design is here: https://youtu.be/w1QcQ_9QPm0


# Documentation Continued #

## Modelling ##

### Original 3D Model ###

![1](https://user-images.githubusercontent.com/62095800/82579587-5f720700-9bd1-11ea-8913-a3d9543950ef.PNG)

![2](https://user-images.githubusercontent.com/62095800/82579593-60a33400-9bd1-11ea-82aa-3b6b773aa972.PNG)

![3](https://user-images.githubusercontent.com/62095800/82579595-613bca80-9bd1-11ea-8f52-a4f341ffd023.PNG)

These are some perspectives on the original 3D model that was the basis of my original cardboard models.

### New 3D Model ###

![4](https://user-images.githubusercontent.com/62095800/82579596-613bca80-9bd1-11ea-9bf9-25f8660a260c.PNG)

![5](https://user-images.githubusercontent.com/62095800/82579601-61d46100-9bd1-11ea-9b54-0ba1d367f5bb.PNG)

![6](https://user-images.githubusercontent.com/62095800/82579602-626cf780-9bd1-11ea-9084-5bf6a042d4a1.PNG)

Due to the feedback received on my previous iterations, I landed on this as a solution. The people I had interact with the original models felt confused and uncomfortable by the hand holds. Based on their physical reactions, I surmised that if the model was wider and the holding positions were at the apex of that width, the shape would fit the contours of the body better. 

### Paper Net of New Model ###

![7](https://user-images.githubusercontent.com/62095800/82579604-626cf780-9bd1-11ea-8e3a-f156437aaa24.jpg)

Let the paper crafts unfortunately begin.

## Technical Testing of Input Methods and Components ##

### Method of Input - Contact Microphone ###

![8](https://user-images.githubusercontent.com/62095800/82579621-6ac53280-9bd1-11ea-9d76-5c91823fe3a9.jpg)

As will be mentioned later on, I had various potential methods of measuring breath and reading it. After some experimentation with a differential pressure sensor, it became clear that it wasn't a viable option. This idea was the second in the list. In essence, the idea was to break apart a contact microphone in order to attach one end of tubing very close to the buzzer element (the piece which vibrates in reaction to loud signals or direct forms of pressure variance). This would, in theory, allow the sensitivity to be lowered so that the only signal would be from the breath entering the tubing. 
Unfortunately, this method wasn't viable as the breath was incapable of sufficiently and consistently providing signal to the microphone. 

### Arduino Fun ###

![9](https://user-images.githubusercontent.com/62095800/82579631-6ef15000-9bd1-11ea-828f-aafc79d3dc67.jpg)

This is just a showing of one of the many complications that came with this project. Due to a lack of personal resources, I attempted to salvage an arduino compatible board from another project board. As you can see, the pins were mangled and ensured problems later on with proper contact and unusable pins. 

### Simple Test of 1/4" Audio Jack###

![10](https://user-images.githubusercontent.com/62095800/82579635-70227d00-9bd1-11ea-97f1-398097de220e.jpg)

Another salvaged component was a 1/4" female audio jack, it worked mostly fine.

### Method of Input - Fan Input ###

![11](https://user-images.githubusercontent.com/62095800/82579776-a233df00-9bd1-11ea-9d1d-e5d0e9e13e11.jpg)

While I did test this method of breath input previously, this test was conducted with the arduino board. It was found to be compatible


## Physical Modelling and Bringing it Together ##

### The Final Cardboard Model ###

![12](https://user-images.githubusercontent.com/62095800/82579818-b081fb00-9bd1-11ea-98bb-8bf4227b8082.jpg)

![13](https://user-images.githubusercontent.com/62095800/82579826-b4ae1880-9bd1-11ea-8abf-f51092f000c8.jpg)

This is the realised cardboard model of the paper net. After being painted, I cut out holes in a pattern and filled them with hot glue. This was to act like frosted acrylic and allow LED light to diffuse but shine through. 

### Installing LEDs and Plugging Everything In ###

![14](https://user-images.githubusercontent.com/62095800/82579838-b8da3600-9bd1-11ea-8b7d-1665866f5373.jpg)

![15](https://user-images.githubusercontent.com/62095800/82579854-bd065380-9bd1-11ea-8be5-eac61e0dc25c.jpg)

![16](https://user-images.githubusercontent.com/62095800/82579871-c1327100-9bd1-11ea-9ade-555486202ce9.jpg)

This process was long and arduous without a soldering iron. During the process of attempting to connect various wires through twisting of contact wires and sealing it with heat shrink tubing, it inevitably created issues in the future on the stability and consistency of the work.

### Deconstruction of Installation ###

![18](https://user-images.githubusercontent.com/62095800/82579882-c55e8e80-9bd1-11ea-82bf-69e89fdd5359.jpg)

After extensive testing and code iterations, my hackney method of contact was found to be fallible. Because of issues I was having with code in general, a decision was made to cut back on the amount of LEDs used and just try to get one working. 

### Full Deconstruction and Addition of Velcro ###

![19](https://user-images.githubusercontent.com/62095800/82579897-ca234280-9bd1-11ea-8677-2e125a2726e5.jpg)

After the complete deconstruction of the board and LEDs, I constructed a platform by which to attach a Velcro strip so that the instrument could be easily opened up from one panel if/when necessary. Additionally, prior to this, I utilised a deconstructed pen to create a kind of structured tubing to direct airflow from the hole (mouth-piece site) towards the fan in such a way that would guarantee that the fan moved. The platform and positioning of the motor and fan was intentional. it's slightly tilted and wedged into the corner so that a tiny amount of the fan blade would graze the edge of the panel. This would create essentially, haptic feedback. 

### Testing of Multiple Boards ###

![20](https://user-images.githubusercontent.com/62095800/82579940-d9a28b80-9bd1-11ea-8c76-d12640f044ac.jpg)

This is after the deconstruction and reconstruction of the board and internal wiring. A problem I consistently had was that the addition of LEDs to the musical code would result in audio LED failure. To try and circumvent this issue, I attempted to take the signal from the arduino board and control the LEDs with the microbit. This eventually resulted in failure.

## Final code ##

```
#include <MozziGuts.h>
#include <Oscil.h> // oscillator
#include <tables/cos2048_int8.h> // table for Oscils to play
#include <AutoMap.h> // maps unpredictable inputs to a range

// desired carrier frequency max and min, for AutoMap
const int MIN_CARRIER_FREQ = 22;
const int MAX_CARRIER_FREQ = 440;

// desired intensity max and min, for AutoMap, note they're inverted for reverse dynamics
const int MIN_INTENSITY = 700;
const int MAX_INTENSITY = 10;

AutoMap kMapCarrierFreq(0,1023,MIN_CARRIER_FREQ,MAX_CARRIER_FREQ);
AutoMap kMapIntensity(0,1023,MIN_INTENSITY,MAX_INTENSITY);

const int KNOB_PIN = 7; // set the input for the knob to analog pin 0
const int LDR_PIN = 7; // set the input for the LDR to analog pin 1

Oscil<COS2048_NUM_CELLS, AUDIO_RATE> aCarrier(COS2048_DATA);
Oscil<COS2048_NUM_CELLS, AUDIO_RATE> aModulator(COS2048_DATA);

int mod_ratio = 3; // harmonics
long fm_intensity; // carries control info from updateControl() to updateAudio()


void setup(){
  //Serial.begin(9600); // for Teensy 3.1, beware printout can cause glitches
  Serial.begin(115200); // set up the Serial output so we can look at the piezo values // set up the Serial output for debugging
  startMozzi(); // :))
}


void updateControl(){
  // read the knob
  int knob_value = mozziAnalogRead(KNOB_PIN); // value is 0-1023

  // map the knob to carrier frequency
  int carrier_freq = kMapCarrierFreq(knob_value);

  //calculate the modulation frequency to stay in ratio
  int mod_freq = carrier_freq * mod_ratio;

  // set the FM oscillator frequencies to the calculated values
  aCarrier.setFreq(carrier_freq);
  aModulator.setFreq(mod_freq);

  // read the light dependent resistor on the Analog input pin
  int light_level= mozziAnalogRead(LDR_PIN); // value is 0-1023

  // print the value to the Serial monitor for debugging
  Serial.print("light_level = ");
  Serial.print(light_level);
  Serial.print("\t"); // prints a tab

  fm_intensity = kMapIntensity(light_level);

  Serial.print("fm_intensity = ");
  Serial.print(fm_intensity);
  Serial.println(); // print a carraige return for the next line of debugging info

}


int updateAudio(){
  long modulation = fm_intensity * aModulator.next();
  return aCarrier.phMod(modulation); // phMod does the FM
}


void loop(){
  audioHook();
}
```

## Design process discussion ##

At the beginning of this journey, I fortunately (but perhaps unintentionally) followed the double diamond methodology by researching and discussing expression through the body. In my field, it is a fascinating subject which tends to focus on the relationship between expression physically through the body but also through the extension thereof into the instrument. The way in which a performer will arguably "unnecessarily" move whilst performing. I didn't have a specific area of interest at the beginning as suggested in the double diamond method (DDM) but a range of specific interests which conglomerate into a confusing mass of questions and theories.

As I started to implement the DDM, I found that it was difficult for me to find the specific "problem" which I was discovering and eventually defining. To help get out of this rut, the best assistance was input from others. I would present some iterations of problems and get wonderful input from a completely new perspective. This started me on the path of human centred design (HCD) as well. 

Throughout the first half of my instrument design and creation, this means of feedback was extremely useful for progress purposes. As I went forward, my tendency was toward the human centred design model. I believe that this because I prefer useful feedback from potential users and/or other designers then integrate that feedback, prototype it and test it again. The looping aspect of HCD is what led me to follow it more closer, still with reference to DDM. While DDM does have elements of feedback looping within the develop and deliver stages, HCD is far more specific on what kind of feedback, by whom and what it is on. When it came to general interactivity and design. I had the aid of roommates to draw from. I was careful as to how I phrased things and what I said my product could be, to ensure less influence. I would ask roommate 1, for example, to hold the instrument in whichever way felt the most comfortable. Through this, I got to see how my instrument would likely be 'discovered' from a UX (user experience) perspective. As an example, this feedback is why the proportions and means of holding were altered. 

I take a more holistic approach to design, which I feel, suits the methodology of HCD quite well. It allows a whole picture to be formed as the process continues. 

As mentioned in the previous journaling, I always intended to focus on the modes of feedback in order to build a unique, product. Specifically, my intentions were to have "audio reactive" lighting and input reactive audio which would be designed in a holistic way to approximate the micro forms of feedback achieved through acoustic instruments. This being something rarely, if ever, achieved by electronic musical instruments. I know that not everyone has the context of digital instrumentation, so let me quickly explain why this is unique. Take the clarinet as an acoustic instrument powered by breath as an exemplar. Due to the physical principles used to generate sound (acoustic resonance of a pipe with one closed end), a specific mouthpiece, buttons an holes were designed to give a user control of the frequencies and (perhaps unintentionally) allow for subtle variations in timbre. This being controlled in near entirety by air speed, amount and embouchure. Those subtle timbral variances create a high level of personality and instrument-user-audience relationship. In the age of electronic and digital instruments, almost all attempts at a breath controlled musical instrument (not a MIDI controller) try to approximate acoustic instruments and neglect the digital possibilities. Even the examples I included show that nearly all breath controlled musical devices aim to emulate acoustic instruments (like the JamBoxx and Aerophone) or simply utilise the interaction method for MIDI control (like the TEC USB and SynthHacker's DIY MIDI controller). My instrument, however, intended to embrace digital sound generation and become more than a digital "version" of an acoustic instrument. 


## Reflection ##

Success is relative. And while I can tell myself that this iteration of *Aeolus* is successful at achieving the core essence of the design intent, I find it a failure in many of the details. The original intent was to have a comprehensive and integrated musical instrument which involved audio reactive LEDs and a more refined musical outcome. This was all to facilitate personality of the product, discoverability and in depth relationship opportunities between the instrument, performer and audience (if any). To be more specific, the design included 12 "frosted windows" which would've diffused light from LEDs. These LEDs were arranged in a colour gradient that would've reacted in brightness to linear increases of range of the fan input. See the *Installing LEDs and Plugging Everything In* section of the documentation images. i.e.

  *Taking into account the range of input being 0 - 1023. Starting from the top mouthpiece and following the curve line
   down: 1 - Blue (0 - 250), 2 - Blue(100 - 300), 3 - Blue(150 - 350), 4 - Green(250 - 450), 5 - Green(300 - 550), 
   6 - Yellow(400 - 650), 7 - Yellow(500 - 700), 8 - Orange(600 - 800), 9 - Orange(700 - 900), 10 - Red(850 - 950), 
   11 - Red(900 - 1023), 12 - Red(975 - 1023)*

This was setup so that as the user blew harder, the LEDs gradually went from blue to red, it becoming more difficult to reach the final LED to parallel the amount of air the user would have to force. While the idea had merit and was novel, interesting and aided both discoverability and feedback, I couldn't get both the audio and visuals to coexist either within the code or wiring. In general, this project presented difficulties at every possible corner. After testing the code with Arduino (I ended up solely working with Arduino IDE and a compatible board) it appeared fine and in working order. As soon as it was transferred to the physical, none of the lights would turn on and the audio suddenly cut. After extensive research and experimentation, I could get some of the LEDs to turn on, but they would not react to the ranges set out or fade (PWM pins were used) or turn off properly. After some time, it became clear that I could not get the LEDs to function as intended and decided that it would be better to get rid of the visual feedback than have defective feedback which would confuse users. There were several more complications with audio reactivity from a single input source with the code. No matter what changes I would implement, nothing significant would change in the results. 

I feel that it is important to describe these complications in order to understand why I consider the final product to be a failure of intention. 

Musically, the final sound generation is also a failure. As mentioned previously, the actual sound product isn't much of a concern in this particular project, but the sound's reaction to the interaction was crucial. In the design process discussion I mentioned that the intention was not the emulate the sound of acoustic instruments but to take the principles of subtle and coherent timbral changes based on one variable of embouchure (there are minute variables within changes in embouchure, but from a macro level of the user, it can be considered as a singular variable). In order to apply these principles, it would require taking the signal from the fan input and manipulating range and scope to utilise in multiple points of modulation. Such as volume, levels of harmonics, blend of different wave shapes and ADSR envelope shape. Just as examples. Every attempt to integrate these controls failed. This is why I consider the audio to be a failure of intent. 

In order to improve upon this project, I feel that some better practices and research be done as to how to achieve an integrated code. Some thoughts on that are to instead of multiple LEDs, utilising a neopixel strip. Instead of attempting to do everything with a single microcontroller, splitting the workload of audio and visuals between two microcontrollers, which I experimented with in the *Testing of Multiple Boards* documentation. 


Novelty is an interesting discussion when it comes to projects and design. There are many different scopes through which you could consider something to be novel or not. For example, my instrument may considered to not be novel because it is an electronic musical instrument, or because it utilises a motor and fan as an input method. On the other hand, I do believe that there are aspects to this project which are novel within the field of electronic wind instruments; such as shape and intention. As mentioned in the design process discussion, most commercial and DIY electronic musical instruments are centred around the sonic emulation of acoustic wind instruments or utilising the breath interaction as a MIDI controller. It is important to point out that a MIDI controller is considered very different to an electronic musical instrument. This is because it only sends values to a patch which either runs a program in real time, or is used to modulate another electronic instrument. Whereas an electronic musical instrument is a self contained, cohesive product that has thought out musical ideas and modulation. This may still include changes to the musical intention, like how the Aerophone can be changed to sound like a cello, tuba or saxophone. When it comes to Aeolus, the intention was never to emulate the sonic qualities of acoustic instruments. To only embrace the electronic sound possibilities (which also negates any comparison to acoustic instruments) but to apply the same principles which allow acoustic instruments to have such subtle variations in timbre, which give them character and complex interactivity.


As far as extensions on this project, I have already been cooking up some ideas. I realised that if electronic visuals weren't included, then some other means of visual feedback would be very useful to the overall discoverability and implementation. Some ideas were to make the housing unit expand and contract to the user's breath (similar to how an accordion "breathes" but purely breath controlled). This would also mean that there would be interactivity on the intake of breath, as well as the exhalation.
